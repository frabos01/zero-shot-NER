{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Authors\n",
        "\n",
        "| Student Name | Student ID | Degree                                |\n",
        "|--------------|------------|----------------------------------------|\n",
        "| Fabrizio Genilotti    | 2119281       | Master Degree in Computer Engineering |\n",
        "| Francesco Boscolo Meneguolo    | 2119969        | Master Degree in Computer Engineering |"
      ],
      "metadata": {
        "id": "v2vBaNAEX45w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Zero-shot NER with Large Language Models\n",
        "\n",
        "In this project the **Falcon-7B-Instruct** [[1](#)] small-size Large Language Model (LLM) is used to perform the Named Entity Recognition (NER) task on the **Few-NERD** [[2](#)] dataset using one of four possible approaches: Vanilla, Decomposed-QA, Tool Augmentation and Salient Entity Span.\n",
        "The model perdormance on each approach is assessed according two types of evaluations: exact matching and token-level evaluation.\n",
        "In the last section are reported the results and the discussion on the results.\n",
        "\n",
        "**Reference bibliography** is in the last cell of the notebook."
      ],
      "metadata": {
        "id": "8EAxIPpf7amK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose NER zero-shot strategy (0 = 'Vanilla', 1 = 'Decomposed-QA', 2 = 'Tool Augmentation' 3 = 'Salient Entity Span')\n",
        "ner_strategy = 2"
      ],
      "metadata": {
        "id": "7eM02POy8sF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwHuXOb-Y38Z"
      },
      "source": [
        "First we need to install the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Ax-iTbZR03"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install hanlp\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Model\n",
        "\n",
        "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B [[1](#)] and finetuned on a mixture of chat/instruct datasets.\n",
        "\n",
        "*For Falcon-7B-Instruct paper coming soon - June 2025*"
      ],
      "metadata": {
        "id": "gAMQJJo78tCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Model name choosen from Hugging Face library\n",
        "model_id = \"tiiuae/Falcon3-7B-Instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model into memory (use GPU if available, else CPU)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Generate using pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "SbMTfK9jgQN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Few-NERD [[2]()] is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. The list of classification labels, with possible values including O (0), art (1), building (2), event (3), location (4), organization (5), other(6), person (7), product (8).\n",
        "\n",
        "It uses **IO** convention schema instead of BIO for evaluation.\n",
        "\n",
        "N.B: label 'O' means **\"not an entity\"**."
      ],
      "metadata": {
        "id": "dPxbv4BRwrVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets fsspec\n",
        "\n",
        "# Dataset loading\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"DFKI-SLT/few-nerd\", \"inter\")"
      ],
      "metadata": {
        "id": "oIHt1ZEerI5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset structure\n",
        "print(dataset)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Dataset instance\n",
        "print(\"Dataset instance:\")\n",
        "print(dataset[\"train\"][1], end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "ghJEcQ7yrSZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition Strategies\n",
        "\n",
        "To perform the Named Entity Recognition (NER) task using an instructed Large Language Model (LLM), four prompting strategies are proposed: Vanilla, Decomposed-QA, Tool Augmentation and Salient Entity Span.\n",
        "\n",
        "- **Vanilla** :\n",
        "\n",
        "  The first approach consists in providing the model with a single structured prompt giving in input the entity label set, the text and the format the model is asked to output.\n",
        "\n",
        "- **Decomposed QA** :\n",
        "\n",
        "  The second approach [[3](#)] decomposes the task into multiple iterations, each focusing on a single entity type. The initial prompt includes the entity label set, the text and the output format. At each iteration, the prompt is expanded by asking to detect the entities belonging to one of the labels from the label set. Once all questions pertaining to each label have been addressed, the conversation is concluded. This strategy aims to narrowing the modelâ€™s focus during each extraction step.\n",
        "\n",
        "- **Tool Augmentation**\n",
        "\n",
        "  The third approach [[3](#)] firstly obtains the syntactic information of the input text via **spaCy** POS tagging tool [[4](#)]. Secondly, it feeds the input text together with the syntactic information to the model. Then, the prompt is iteratively expanded as in Decomposed QA.\n",
        "\n",
        "- **Salien Entity Span** :\n",
        "\n",
        "  The fourth approach breaks the task in two stages. In the first step, the model is prompted with the input text and is asked to output a list of potential salient entity spans. In the second step the model is re-prompted to recognize the entity spans given the entity label set, the text, the potential entity spans and the asked output format.  This approach separates span detection from classification, potentially improving accuracy by allowing more focused and specialized reasoning in each stage.\n",
        "\n",
        "The dataset is used to test each of these strategies. Since the test process have a heavy computational cost for this setting the **test is made over 50 dataset instances**.\n",
        "\n",
        "**N.B.:** validation instances (used in debugging phase) are different from test instances"
      ],
      "metadata": {
        "id": "H2-EKve6H0ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O (0), art (1), building (2), event (3), location (4), organization (5), other(6), person (7), product (8).\n",
        "# Entity labels of interest, O excluded\n",
        "entity_labels = [\"art\", \"building\", \"event\", \"location\", \"organization\", \"other\", \"person\", \"product\"]\n",
        "\n",
        "# Store results in list\n",
        "results = []\n",
        "\n",
        "# Number of test instances\n",
        "test_size = 50"
      ],
      "metadata": {
        "id": "V4Uf92Q8UcTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla"
      ],
      "metadata": {
        "id": "hAR3Cvi6ZPRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Convert into multiple dictionaries, one per each possible entity tag\n",
        "def to_dialogue_stages(raw_output_str, entity_labels):\n",
        "    # Remove <|assistant|> and whitespace\n",
        "    if \"<|assistant|>\" in raw_output_str:\n",
        "      cleaned = raw_output_str.split(\"<|assistant|>\", 1)[-1].strip()\n",
        "    else:\n",
        "      cleaned = raw_output_str.strip()\n",
        "\n",
        "    # Fix improperly formatted dictionary-like output (e.g., ['key': 'value'])\n",
        "    if cleaned.startswith(\"[\") and \":\" in cleaned:\n",
        "        cleaned = cleaned.replace(\"[\", \"{\").replace(\"]\", \"}\")\n",
        "\n",
        "    entity_dict = {}\n",
        "\n",
        "    # String to dictionary\n",
        "    entity_dict = ast.literal_eval(cleaned)\n",
        "\n",
        "    label_to_entities = {label: [] for label in entity_labels}\n",
        "\n",
        "    # Insert entity in correct dicionary\n",
        "    for entity, label in entity_dict.items():\n",
        "        if label in label_to_entities:\n",
        "            label_to_entities[label].append(entity)\n",
        "\n",
        "    return [{'label': label, 'raw_answer': label_to_entities[label]} for label in entity_labels]\n",
        "\n",
        "# Vanilla NER zero-shot strategy\n",
        "if ner_strategy == 0:\n",
        "  # Loop over train corpus\n",
        "  for i, sample in enumerate(dataset[\"train\"]):\n",
        "      # Prepare input text\n",
        "      text_input = \" \".join(sample[\"tokens\"]).replace(\" .\", \".\").replace(\" ,\", \",\").replace(\"'\", \" \")\n",
        "      print(i)\n",
        "\n",
        "      # Safely quote the text using JSON\n",
        "      quoted_text_input = json.dumps(text_input)\n",
        "\n",
        "      # Build a single prompt for all entity types\n",
        "      prompt = f\"\"\"Given the entity label set: {{art, building, event, location, organization, other, person, product}}. Based on the given entity label set, please recognize the named entities in the given text. Text: '{quoted_text_input}'. Answer has to be dictionary like: {{'entity_name': 'label'}}. Answer:\"\"\"\n",
        "\n",
        "      # Generate output from model\n",
        "      output = pipe(prompt, max_new_tokens=1000, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "      # Extract answer after the \"Answer:\"\n",
        "      entity_dict_text = output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "      # Store the result\n",
        "      result = {\n",
        "          \"text_index\": i,\n",
        "          \"text_input\": text_input,\n",
        "          \"dialogue_stages\": to_dialogue_stages(entity_dict_text, entity_labels)\n",
        "      }\n",
        "\n",
        "      results.append(result)\n",
        "\n",
        "      # Iterate over test_size instances\n",
        "      if i == (test_size - 1):\n",
        "        print(\"NER System: end\")\n",
        "        break"
      ],
      "metadata": {
        "id": "F13vMT_qwWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decomposed QA"
      ],
      "metadata": {
        "id": "piyEmereZRc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import ast\n",
        "\n",
        "# Extract dictionaries of entities\n",
        "def normalize_raw_answer(raw_answer, current_label):\n",
        "    # Remove <|assistant|> and whitespace\n",
        "    cleaned = re.sub(r\"<\\|assistant\\|>\\s*\", \"\", raw_answer.strip())\n",
        "\n",
        "    # Treat empty string as empty dict\n",
        "    if cleaned in (\"\", \"{}\"):\n",
        "        return {}\n",
        "\n",
        "    parsed = ast.literal_eval(cleaned)\n",
        "\n",
        "    # Handle cases in which the model output different formats\n",
        "    if isinstance(parsed, dict):\n",
        "        for v in parsed.values():\n",
        "            if isinstance(v, list):\n",
        "                return v\n",
        "    elif isinstance(parsed, list):\n",
        "        candidates = parsed\n",
        "\n",
        "    return candidates\n",
        "\n",
        "# Decomposed-QA NER zero-shot strategy\n",
        "if ner_strategy == 1:\n",
        "  # Loop over train corpus\n",
        "  for i, sample in enumerate(dataset[\"train\"]):\n",
        "    # Input text\n",
        "    text_input = \" \".join(sample[\"tokens\"]).replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "    print(i)\n",
        "\n",
        "    # Safely quote the text using JSON\n",
        "    quoted_text_input = json.dumps(text_input)\n",
        "\n",
        "    # Initialize dialogue conversation\n",
        "    dialogue =  f\"\"\"Based on the given entity label set {{art, building, event, location, organization, other, person, product}}, please recognize the named entities in the given text. Text: '{quoted_text_input}'. Answer has to be like in JSON format {{'label': []}}.\"\"\"\n",
        "\n",
        "    # Store dialogue Q&A of current text input\n",
        "    result = {\n",
        "        \"text_index\": i,\n",
        "        \"text_input\": text_input,\n",
        "        \"dialogue_stages\": []\n",
        "    }\n",
        "\n",
        "    for label in entity_labels:\n",
        "      # Build prompt\n",
        "      dialogue = dialogue + f\"\"\" Question: what are the named entities as '{label}' in the text?.\n",
        "      Answer:\"\"\"\n",
        "\n",
        "      # Generate model output\n",
        "      output = pipe(dialogue, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "      # Extract last answer\n",
        "      last_answer = output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "      # Append the answer relative to last question\n",
        "      dialogue = dialogue + f\"{last_answer}\"\n",
        "\n",
        "      # Store result\n",
        "      result[\"dialogue_stages\"].append({\n",
        "          \"label\": label,\n",
        "          \"raw_answer\": normalize_raw_answer(last_answer, label)\n",
        "      })\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "    # Iterate over val_size instances\n",
        "    if i == (test_size - 1):\n",
        "      print(\"NER System: end\")\n",
        "      break"
      ],
      "metadata": {
        "id": "UjuTIG6QBaUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool Augmentation"
      ],
      "metadata": {
        "id": "4CAuHowYZWuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import ast\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "# Tool Augmentation\n",
        "if ner_strategy == 2:\n",
        "    # spacy POS tagging model\n",
        "    spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Loop over train corpus\n",
        "    for i, sample in enumerate(dataset[\"train\"]):\n",
        "      # Input text\n",
        "      text_input = \" \".join(sample[\"tokens\"]).replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "      print(f\"Instance {i}\")\n",
        "\n",
        "      # Safely quote the text using JSON\n",
        "      quoted_text_input = json.dumps(text_input)\n",
        "\n",
        "      # Process the text with spaCy\n",
        "      doc = spacy_pipeline(quoted_text_input)\n",
        "\n",
        "      # Extract tokens (including punctuation) and their POS tags\n",
        "      tokens = [token.text for token in doc]\n",
        "      pos_tags = [token.pos_ for token in doc]  # Coarse-grained POS\n",
        "\n",
        "      pos_result = ' '.join(f'{a}/{b}' for a, b in zip(tokens, pos_tags))\n",
        "\n",
        "      # Initialize dialogue conversation\n",
        "      dialogue =  f\"\"\"Given entity label set: {{art, building, event, location, other, person, product}}. Given the text and the corresponding Part-of-Speech tags, please recognize the named entities in the given text. Text: '{quoted_text_input}'. Part-of-Speech tags: '{pos_result}'. Answer has to be like in JSON format {{'label': []}}.\"\"\"\n",
        "\n",
        "      # Store dialogue Q&A of current text input\n",
        "      result = {\n",
        "          \"text_index\": i,\n",
        "          \"text_input\": text_input,\n",
        "          \"dialogue_stages\": []\n",
        "      }\n",
        "\n",
        "      for label in entity_labels:\n",
        "        # Build prompt\n",
        "        dialogue = dialogue + f\"\"\" Question: what are the named entities as '{label}' in the text?.\n",
        "        Answer:\"\"\"\n",
        "        print(label)\n",
        "        # Generate model output\n",
        "        output = pipe(dialogue, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "        # Extract last answer\n",
        "        last_answer = output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Append the answer relative to last question\n",
        "        dialogue = dialogue + f\"{last_answer}\"\n",
        "\n",
        "        # Store result\n",
        "        result[\"dialogue_stages\"].append({\n",
        "            \"label\": label,\n",
        "            \"raw_answer\": normalize_raw_answer(last_answer, label)\n",
        "        })\n",
        "\n",
        "      results.append(result)\n",
        "\n",
        "      # Iterate over val_size instances\n",
        "      if i == (test_size - 1):\n",
        "        print(\"NER System: end\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "bS_T_bx48XjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salient Entity Span"
      ],
      "metadata": {
        "id": "k7V6FudOZY19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import ast\n",
        "import json\n",
        "\n",
        "# Extract entity spans from the list of salient spans\n",
        "def extract_entities(text_entities):\n",
        "    match = re.search(r\"\\[.*\\]\", text_entities, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return ast.literal_eval(match.group(0))\n",
        "        except:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "# Convert into multiple dictionaries, one per each possible entity tag\n",
        "def to_dialogue_stages_spans(raw_output_str, entity_labels):\n",
        "    # Remove <|assistant|> and whitespace\n",
        "    if \"<|assistant|>\" in raw_output_str:\n",
        "      cleaned = raw_output_str.split(\"<|assistant|>\", 1)[-1].strip()\n",
        "    else:\n",
        "      cleaned = raw_output_str.strip()\n",
        "\n",
        "    entity_dict = {}\n",
        "\n",
        "    # String to dictionary\n",
        "    entity_dict = ast.literal_eval(cleaned)\n",
        "\n",
        "    # Normalize extracted entity dict\n",
        "    normalized_dict = {}\n",
        "    for entity, label in entity_dict.items():\n",
        "      if isinstance(label, set) or isinstance(label, list):\n",
        "          # Handle set or list of labels\n",
        "          normalized_dict[entity] = next(iter(label), None)\n",
        "      elif isinstance(label, dict) and 'label' in label:\n",
        "          normalized_dict[entity] = label['label']\n",
        "      else:\n",
        "          normalized_dict[entity] = label\n",
        "\n",
        "    label_to_entities = {label: [] for label in entity_labels}\n",
        "\n",
        "    # Insert entity in correct dicionary\n",
        "    for entity, label in normalized_dict.items():\n",
        "        if label in label_to_entities:\n",
        "            label_to_entities[label].append(entity)\n",
        "\n",
        "    return [{'label': label, 'raw_answer': label_to_entities[label]} for label in entity_labels]\n",
        "\n",
        "\n",
        "# Custom NER zero-shot strategy\n",
        "if ner_strategy == 3:\n",
        "  # Loop over train corpus\n",
        "  for i, sample in enumerate(dataset[\"train\"]):\n",
        "    # Input text\n",
        "    text_input = \" \".join(sample[\"tokens\"]).replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "    print(i)\n",
        "\n",
        "    # Safely quote the text using JSON\n",
        "    quoted_text_input = json.dumps(text_input)\n",
        "\n",
        "    # Salient span highlighting\n",
        "    prompt = f\"\"\"Extract all named entities from the following text. Return them as a JSON list of strings (no labels yet). Only include unique, meaningful names or phrases that refer to people, places, organizations, or other named things. Text: '{quoted_text_input}'. Answer:\"\"\"\n",
        "\n",
        "    # Generate model output\n",
        "    output_entities = pipe(prompt, max_new_tokens=1000, do_sample=False)[0][\"generated_text\"]\n",
        "    entity_list = json.dumps(extract_entities(output_entities))\n",
        "\n",
        "    prompt = f\"\"\"Based on the given entity label set {{art, building, event, location, organization, other, person, product}} and extracted entities, please recognize the named entities in the given text. Text: '{quoted_text_input}'. Entities: {entity_list}. Answer has to be like in JSON format, containing key entity and value label. Answer:\"\"\"\n",
        "\n",
        "    # Generate model output\n",
        "    output = pipe(prompt, max_new_tokens=1000, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    # Extract answer after the \"Answer:\"\n",
        "    entity_dict_text = output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Store the result\n",
        "    result = {\n",
        "        \"text_index\": i,\n",
        "        \"text_input\": text_input,\n",
        "        \"dialogue_stages\": to_dialogue_stages_spans(entity_dict_text, entity_labels)\n",
        "    }\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "    # Iterate over val_size instances\n",
        "    if i == (test_size - 1):\n",
        "      print(\"NER System: end\")\n",
        "      break"
      ],
      "metadata": {
        "id": "Y9WIacLrxB96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity Post-processing\n",
        "\n",
        "Before performing evaluation, the model predictions are post-processed.\n",
        "\n",
        "For each dataset instance, the model prediction is parsed to extract entity spans in a structured format. Each entity span is defined as a tuple of the form `(text, (start_idx, end_idx), label)`. Where:\n",
        "\n",
        "- `text` : the entity text span, as it appears in the input\n",
        "- `(start_idx, end_idx)` : the span boundaries indicating start and end token indices\n",
        "- `label` : the gold/predicted entity type\n",
        "\n"
      ],
      "metadata": {
        "id": "X3NbqLzwA0eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_labels = [\"O\", \"art\", \"building\", \"event\", \"location\", \"organization\", \"other\", \"person\", \"product\"]"
      ],
      "metadata": {
        "id": "TesTytglP221"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract dataset training instance in IO convention (not BIO)\n",
        "def extract_io_entities(tokens, tags, label_map):\n",
        "    \"\"\"\n",
        "    Converts an IO-tagged sequence into entity spans.\n",
        "    Each span is (text, (start_idx, end_idx), label)\n",
        "    \"\"\"\n",
        "\n",
        "    # Store entity tuples\n",
        "    entities = []\n",
        "\n",
        "    # Current entity text data\n",
        "    current_entity = []\n",
        "    current_label = None\n",
        "    start_idx = None\n",
        "\n",
        "    # Process each token with its tag\n",
        "    for idx, tag in enumerate(tags):\n",
        "      if tag != 0:\n",
        "        label = label_map[tag]\n",
        "\n",
        "        # Check if token is in current entity (else start new entity)\n",
        "        if current_label == label:\n",
        "          current_entity.append(tokens[idx])\n",
        "        else:\n",
        "          if current_entity:\n",
        "            span = (start_idx, idx)\n",
        "            entities.append((\" \".join(current_entity), span, current_label))\n",
        "          current_entity = [tokens[idx]]\n",
        "          current_label = label\n",
        "          start_idx = idx\n",
        "      else:\n",
        "        # Save entity when next tag is 'O'\n",
        "        if current_entity:\n",
        "          span = (start_idx, idx)\n",
        "          entities.append((\" \".join(current_entity), span, current_label))\n",
        "          current_entity = []\n",
        "          current_label = None\n",
        "          start_idx = None\n",
        "\n",
        "    # Last entity handle\n",
        "    if current_entity:\n",
        "        span = (start_idx, len(tokens))\n",
        "        entities.append((\" \".join(current_entity), span, current_label))\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "8NT-RpybzbCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract predicted LLM instances in IO convention\n",
        "def extract_llm_entities(llm_output, tokens):\n",
        "    \"\"\"\n",
        "    Converts an IO-tagged sequence into entity spans.\n",
        "    Each span is (text, (start_idx, end_idx), label)\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Process each entity recognized by LLM\n",
        "    for entry in llm_output:\n",
        "      label = entry[\"label\"]\n",
        "      raw_answer = entry[\"raw_answer\"]\n",
        "\n",
        "      # Check if dict is not empty\n",
        "      if not raw_answer:\n",
        "        continue\n",
        "\n",
        "      for entity_text in raw_answer:\n",
        "          # Get each token of current entity\n",
        "          entity_tokens = entity_text.split()\n",
        "\n",
        "          for i in range(len(tokens)):\n",
        "            if tokens[i : i + len(entity_tokens)] == entity_tokens:\n",
        "              span = (i, i + len(entity_tokens))\n",
        "              predictions.append((\" \".join(tokens[i:span[1]]), span, label))\n",
        "              break\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "OOHagGit91Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "For the evaluation process the model relies on two approaches:\n",
        "- **Exact evaluation** (coarse-grained)\n",
        "\n",
        "  This method assesses the model's ability to identfy and localize complete entity spans with the correct labels. A predicted entity is considered correct only if it matches the ground truth exactly in both span boundaries and entity type. Partial matches or mislabelings are treated as errors.\n",
        "\n",
        "- **Token-level evaluation** (fine-grained)\n",
        "\n",
        "  This method assesses the model's performance at the level of individual tokens by comparing predicted and ground truth labels. Entity spans are converted into token-wise IO labels, excluding the 'O' label representing non-entity tokens. This approach captures how well the model labels tokens even when full entity spans are not perfectly matched.\n",
        "\n",
        "### **Micro, Macro and Weighted average**\n",
        "\n",
        "For the **Token-level evaluation**, three types of averaging methods are considered for the chosen metrics:\n",
        "\n",
        "- **Micro average**: it computes metrics by aggregating the contributions of all classes. It treats every instance equally, regardless of class, by summing up the true positives, false positives, and false negatives across all classes before calculating the metric.\n",
        "\n",
        "  **This averaging method is the main focus of the evaluation.**\n",
        "\n",
        "- **Macro average**: it computes the metrics independently for each class and then takes the unweighted mean. It treats all classes equally, regardless of their frequency.\n",
        "\n",
        "- **Weighted average**: it computes metrics independently for each class, and then takes the mean weighted by the number of true instances in each class. This accounts for class imbalance by giving more influence to more frequent classes."
      ],
      "metadata": {
        "id": "GVZjVRFLEjOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible entity tags\n",
        "entity_labels = [\"O\", \"art\", \"building\", \"event\", \"location\", \"organization\", \"other\", \"person\", \"product\"]"
      ],
      "metadata": {
        "id": "z3x6G3Q3AHgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXACT EVALUATION\n",
        "\n",
        "def exact_evaluation(results, dataset, entity_labels, start_index, end_index):\n",
        "  # Initialize counters\n",
        "  total_tp = 0\n",
        "  total_fp = 0\n",
        "  total_fn = 0\n",
        "\n",
        "  # Loop over instances for computing exact match metrics\n",
        "  for i in range(start_index, end_index + 1):\n",
        "\n",
        "      tokens = dataset[\"train\"][i][\"tokens\"]\n",
        "\n",
        "      # Extract true and predicted entities\n",
        "      true_entities = extract_io_entities(tokens, dataset[\"train\"][i][\"ner_tags\"], entity_labels)\n",
        "      pred_entities = extract_llm_entities(results[i][\"dialogue_stages\"], tokens)\n",
        "\n",
        "      # Convert to sets for exact match comparison (IO format)\n",
        "      true_set = set(true_entities)\n",
        "      pred_set = set(pred_entities)\n",
        "\n",
        "      # Compute counts\n",
        "      tp = len(true_set & pred_set)\n",
        "      fp = len(pred_set - true_set)\n",
        "      fn = len(true_set - pred_set)\n",
        "\n",
        "      # Accumulate totals\n",
        "      total_tp += tp\n",
        "      total_fp += fp\n",
        "      total_fn += fn\n",
        "\n",
        "  # Compute final precision, recall, F1\n",
        "  precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "  recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "  f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "  # Final metric scores (exact match)\n",
        "  metric_results = {\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1,\n",
        "      \"true_positives\": total_tp,\n",
        "      \"false_positives\": total_fp,\n",
        "      \"false_negatives\": total_fn,\n",
        "  }\n",
        "\n",
        "  return metric_results\n",
        "\n",
        "# Exact match computation\n",
        "metric_results = exact_evaluation(results, dataset, entity_labels, 0, 49)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Exact evaluation - Evaluation Results:\")\n",
        "print(f\"Precision: {metric_results['precision']:.3f}\")\n",
        "print(f\"Recall:    {metric_results['recall']:.3f}\")\n",
        "print(f\"F1 Score:  {metric_results['f1']:.3f}\")\n",
        "print(f\"TP: {metric_results['true_positives']}, FP: {metric_results['false_positives']}, FN: {metric_results['false_negatives']}\")"
      ],
      "metadata": {
        "id": "QPgB1f0xFKy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ScM3g4KcfgOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKEN-LEVEL EVALUATION\n",
        "\n",
        "# Convert entity span to label sequence over entire sentence\n",
        "def spans_to_io_labels(tokens, spans):\n",
        "    labels = ['O'] * len(tokens)\n",
        "    for _, (start_idx, end_idx), ent_type in spans:\n",
        "        for i in range(start_idx, end_idx):\n",
        "            labels[i] = f\"{ent_type}\"\n",
        "    return labels\n",
        "\n",
        "def token_level_evaluation(results, dataset, entity_labels, start_index, end_index):\n",
        "  # Data structures for metric evaluation\n",
        "  all_true_labels = []\n",
        "  all_pred_labels = []\n",
        "\n",
        "  for i in range(start_index, end_index + 1):\n",
        "      tokens = dataset[\"train\"][i][\"tokens\"]\n",
        "\n",
        "      # Extract entities\n",
        "      ground_truth = extract_io_entities(tokens, dataset[\"train\"][i][\"ner_tags\"], entity_labels)\n",
        "      predicted = extract_llm_entities(results[i][\"dialogue_stages\"], tokens)\n",
        "\n",
        "      # Convert spans to token-level labels\n",
        "      true_labels = spans_to_io_labels(tokens, ground_truth)\n",
        "      pred_labels = spans_to_io_labels(tokens, predicted)\n",
        "\n",
        "      # Aggregate\n",
        "      all_true_labels.extend(true_labels)\n",
        "      all_pred_labels.extend(pred_labels)\n",
        "\n",
        "  return all_pred_labels, all_true_labels\n",
        "\n",
        "# Token-level evaluation\n",
        "all_pred_labels, all_true_labels = token_level_evaluation(results, dataset, entity_labels, 0, 49)\n",
        "\n",
        "# Extract lables present in data\n",
        "labels = list(set(all_true_labels + all_pred_labels) - {'O'})\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(all_true_labels, all_pred_labels, labels=labels, zero_division=0)\n",
        "print(\"Token-level evaluation - Evaluation Results:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "1YeWdnaTAMVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation results\n",
        "\n",
        "### **Vanilla**\n",
        "The method shows limited capability in detecting entity spans, as suggested by its low performance metrics. It achieves a low precision, reflecting a high number of false positives, and a moderately higher recall, indicating a better but still limited ability to identify true entities and reduce false negatives. Overall, the F1 score underscores the need for improvement in both precision and recall.\n",
        "\n",
        "#### Exact Match\n",
        "```\n",
        "Precision: 0.279\n",
        "Recall:    0.510\n",
        "F1 Score:  0.361\n",
        "TP: 53, FP: 137, FN: 51\n",
        "```\n",
        "\n",
        "### Token-level\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    building       0.09      0.17      0.12        23\n",
        "      person       0.28      0.73      0.40        11\n",
        "organization       0.68      0.51      0.58        63\n",
        "     product       0.40      0.86      0.55         7\n",
        "       other       0.00      0.00      0.00         0\n",
        "    location       0.52      0.60      0.56        73\n",
        "       event       0.00      0.00      0.00         4\n",
        "\n",
        "   micro avg       0.29      0.52      0.37       181 <--\n",
        "   macro avg       0.28      0.41      0.31       181\n",
        "weighted avg       0.49      0.52      0.49       181\n",
        "```\n",
        "\n",
        "### **Decomposed-QA**\n",
        "The performance of this method is comparable to that of the Vanilla approach. It exhibits a sligltly higher precision and a slightly higher recall. This suggests that the model tends to identify more entities within the sentences, including correct ones. However, it identifies more incorrect or spurious entities that are not actually present (false positives).\n",
        "\n",
        "#### Exact Match\n",
        "```\n",
        "Precision: 0.283\n",
        "Recall:    0.577\n",
        "F1 Score:  0.380\n",
        "TP: 60, FP: 152, FN: 44\n",
        "```\n",
        "\n",
        "#### Token-level\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     product       0.00      0.00      0.00         7\n",
        "    building       0.00      0.00      0.00        23\n",
        "       event       0.00      0.00      0.00         4\n",
        "       other       0.00      0.00      0.00         0\n",
        "    location       0.39      0.74      0.51        73\n",
        "         art       0.00      0.00      0.00         0\n",
        "      person       0.26      0.91      0.41        11\n",
        "organization       0.46      0.62      0.53        63\n",
        "\n",
        "   micro avg       0.28      0.57      0.38       181 <--\n",
        "   macro avg       0.14      0.28      0.18       181\n",
        "weighted avg       0.33      0.57      0.41       181\n",
        "```\n",
        "\n",
        "### **Tool augmentation**\n",
        "This the worst approach in terms of results. The POS taggings do not help the model to perform better, it actually worsens in all metrics (both in exact matching and token-level).\n",
        "#### Exact Match\n",
        "```\n",
        "Precision: 0.202\n",
        "Recall:    0.481\n",
        "F1 Score:  0.284\n",
        "TP: 50, FP: 198, FN: 54\n",
        "```\n",
        "\n",
        "### Token-level\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    location       0.36      0.70      0.48        73\n",
        "    building       0.00      0.00      0.00        23\n",
        "      person       0.25      0.91      0.39        11\n",
        "       other       0.00      0.00      0.00         0\n",
        "organization       0.43      0.38      0.40        63\n",
        "       event       0.03      0.25      0.05         4\n",
        "     product       0.21      0.43      0.29         7\n",
        "         art       0.00      0.00      0.00         0\n",
        "\n",
        "   micro avg       0.25      0.49      0.33       181 <--\n",
        "   macro avg       0.16      0.33      0.20       181\n",
        "weighted avg       0.32      0.49      0.37       181\n",
        "```\n",
        "\n",
        "### **Salient entity span**\n",
        "This method demonstrates almost a moderate ability to correctly identify entities, with a better balance between precision and recall with respect to the other methods. While recall is slightly lower than **Decomposed QA** approach, precision is significantly higher, indicating that the model makes fewer false positive predictions, thus producing more reliable outputs.\n",
        "\n",
        "#### Exact Match\n",
        "```\n",
        "Precision: 0.415\n",
        "Recall:    0.519\n",
        "F1 Score:  0.462\n",
        "TP: 54, FP: 76, FN: 50\n",
        "```\n",
        "\n",
        "#### Token-level\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    location       0.45      0.63      0.52        73\n",
        "    building       0.05      0.09      0.07        23\n",
        "      person       0.31      0.73      0.43        11\n",
        "       other       0.00      0.00      0.00         0\n",
        "       event       0.00      0.00      0.00         4\n",
        "     product       0.38      0.71      0.50         7\n",
        "organization       0.72      0.54      0.62        63\n",
        "         art       0.00      0.00      0.00         0\n",
        "\n",
        "   micro avg       0.37      0.52      0.44       181 <--\n",
        "   macro avg       0.24      0.34      0.27       181\n",
        "weighted avg       0.47      0.52      0.48       181\n",
        "```"
      ],
      "metadata": {
        "id": "KDRLstwjv0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "Overall, the model demonstrates limited capability in solving the Named Entity Recognition (NER) task across most methods. However, it achieves almost moderate performance with the Salient Entity Span approach, which stands out as the most effective.\n",
        "\n",
        "Notably, even though the **Falcon-7B-Instruct** model was not fine-tuned for NER, it still shows promising results when paired with the right method. These results are particularly noteworthy given both the small size of the model and the use of a general-domain dataset, which increases the complexity of the task."
      ],
      "metadata": {
        "id": "oKerNtz3PYeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. **Almazroue et al.**  <br />\n",
        "  *The Falcon Series of Open Language Models* <br />\n",
        "  [https://arxiv.org/abs/2311.16867](https://arxiv.org/abs/2311.16867)<br />\n",
        "  *For Falcon-7B-Instruct paper coming soon - June 2025* <br />\n",
        "  **Model:** [https://huggingface.co/tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)\n",
        "\n",
        "2. **Ning Ding et al.**  \n",
        "   *Few-NERD: A Few-shot Named Entity Recognition Dataset*.  \n",
        "   [https://arxiv.org/abs/2105.07464](https://arxiv.org/abs/2105.07464)\n",
        "\n",
        "3. **Tingyu Xie et al.**  \n",
        "  *Empirical Study of Zero-Shot NER with ChatGPT*. 2023.  \n",
        "  [https://arxiv.org/abs/2310.10035](https://arxiv.org/abs/2310.10035)\n",
        "\n",
        "4. **spaCy**  \n",
        "  *Part-of-speech tagging*.  \n",
        "  [https://spacy.io/usage/linguistic-features#pos-tagging](https://spacy.io/usage/linguistic-features#pos-tagging)"
      ],
      "metadata": {
        "id": "3Qb-pn4tdsQN"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}